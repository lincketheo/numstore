# Introducing Numstore: A Database for the Machine Learning Era

I've been working on numstore for the past year, and I think it's time to talk about why it exists.

# The Problem
- Consider these use cases:

I want to access the i, j, k-th pixel of a list of images sized 256x256
I want to pull slices of time-series sensor data across multiple dimensions
I want to efficiently store and query multi-dimensional numerical arrays

A relational database wouldn't handle this very well. Sure, there are binary blobs, but those aren't treated as first-class citizens. You end up serializing/deserializing entire objects just to access a single element. It's clunky, slow, and fundamentally not designed for how machine learning workloads actually operate.
Numpy and Data Locality
Here's the thing: Python stores lists as a series of objects (PyObject's in C). That's fine for general-purpose programming, but terrible for numerical computation. Numpy was written because it just makes sense to store contiguous data in binary-packed streams.
Numstore is to Postgres as Numpy is to Python arrays. It's a step closer to the way machines actually process tightly-packed data.
Right now, numstore achieves 640+ MB/s sustained write speeds using novel R+Tree data structures and ARIES WAL recovery. But the vision goes deeper than just performance.
A Sketch for the Future
I originally set out with a complex and rich type system, but realized that database development is HARD. So I decided to skim that stuff off for now. There are remnantsâ€”check out nscompiler for a type compiler. I'd like to implement this fully in the future (see nsdblite), but for now, it's just bytes.
Here's what I had in mind:
Type System

